{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b704273-7bb4-4e76-a4e7-2700fe3efee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup for Hybrid RAG - The Meta-RAG System\n",
    "# Hybrid RAG combines multiple RAG strategies in a single intelligent workflow\n",
    "# Requires both OpenAI and Tavily API keys for comprehensive retrieval capabilities\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load API keys for multi-source retrieval\n",
    "# OpenAI: For LLM operations (scoring, rewriting, synthesis)\n",
    "# Tavily: For web search when local knowledge is insufficient\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Validate required API keys\n",
    "# Both are essential for Hybrid RAG's comprehensive approach\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file.\")\n",
    "if not tavily_api_key:\n",
    "    raise ValueError(\"TAVILY_API_KEY not found in .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3eb21b-033c-4040-b472-b439c3c99919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries for Hybrid RAG Implementation\n",
    "# Hybrid RAG is the most comprehensive approach, combining all RAG strategies\n",
    "\n",
    "# Core data processing and ML libraries\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# LangChain document processing components\n",
    "# Note: Some imports are duplicated - could be cleaned up in production\n",
    "from langchain.document_loaders import DataFrameLoader  # Document loading\n",
    "from langchain.text_splitter import CharacterTextSplitter  # Text chunking\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # Legacy import\n",
    "from langchain.vectorstores import FAISS  # Legacy import\n",
    "from langchain_community.vectorstores import FAISS  # Current import\n",
    "from langchain_community.document_loaders import DataFrameLoader  # Current import\n",
    "\n",
    "# LangChain core workflow components\n",
    "from langchain_core.prompts import ChatPromptTemplate  # Prompt templates\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough  # Chain composition\n",
    "from langchain_core.output_parsers import StrOutputParser  # Response parsing\n",
    "from langchain import hub  # Prompt hub access\n",
    "from langchain_openai import ChatOpenAI  # OpenAI integration\n",
    "\n",
    "# Additional ML and utility libraries\n",
    "import os\n",
    "import getpass\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from langchain.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920ba51-ec0a-4c16-9d0b-8695effe0f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 265, which is longer than the specified 200\n",
      "Created a chunk of size 1411, which is longer than the specified 200\n",
      "Created a chunk of size 294, which is longer than the specified 200\n",
      "Created a chunk of size 395, which is longer than the specified 200\n",
      "Created a chunk of size 298, which is longer than the specified 200\n",
      "Created a chunk of size 324, which is longer than the specified 200\n",
      "Created a chunk of size 470, which is longer than the specified 200\n",
      "Created a chunk of size 662, which is longer than the specified 200\n",
      "Created a chunk of size 451, which is longer than the specified 200\n",
      "Created a chunk of size 245, which is longer than the specified 200\n",
      "Created a chunk of size 226, which is longer than the specified 200\n",
      "Created a chunk of size 580, which is longer than the specified 200\n",
      "Created a chunk of size 1970, which is longer than the specified 200\n",
      "Created a chunk of size 617, which is longer than the specified 200\n",
      "Created a chunk of size 312, which is longer than the specified 200\n",
      "Created a chunk of size 304, which is longer than the specified 200\n",
      "Created a chunk of size 389, which is longer than the specified 200\n",
      "Created a chunk of size 697, which is longer than the specified 200\n",
      "Created a chunk of size 841, which is longer than the specified 200\n",
      "Created a chunk of size 501, which is longer than the specified 200\n",
      "Created a chunk of size 743, which is longer than the specified 200\n",
      "Created a chunk of size 787, which is longer than the specified 200\n",
      "Created a chunk of size 1002, which is longer than the specified 200\n",
      "Created a chunk of size 239, which is longer than the specified 200\n",
      "Created a chunk of size 205, which is longer than the specified 200\n",
      "Created a chunk of size 950, which is longer than the specified 200\n",
      "Created a chunk of size 622, which is longer than the specified 200\n",
      "Created a chunk of size 632, which is longer than the specified 200\n",
      "Created a chunk of size 215, which is longer than the specified 200\n",
      "Created a chunk of size 394, which is longer than the specified 200\n",
      "Created a chunk of size 254, which is longer than the specified 200\n",
      "Created a chunk of size 335, which is longer than the specified 200\n",
      "Created a chunk of size 356, which is longer than the specified 200\n",
      "Created a chunk of size 362, which is longer than the specified 200\n",
      "Created a chunk of size 767, which is longer than the specified 200\n",
      "Created a chunk of size 314, which is longer than the specified 200\n",
      "Created a chunk of size 462, which is longer than the specified 200\n",
      "Created a chunk of size 478, which is longer than the specified 200\n",
      "Created a chunk of size 241, which is longer than the specified 200\n",
      "Created a chunk of size 734, which is longer than the specified 200\n",
      "Created a chunk of size 487, which is longer than the specified 200\n",
      "Created a chunk of size 565, which is longer than the specified 200\n",
      "Created a chunk of size 336, which is longer than the specified 200\n",
      "Created a chunk of size 265, which is longer than the specified 200\n",
      "Created a chunk of size 329, which is longer than the specified 200\n",
      "Created a chunk of size 310, which is longer than the specified 200\n",
      "Created a chunk of size 894, which is longer than the specified 200\n",
      "Created a chunk of size 443, which is longer than the specified 200\n",
      "Created a chunk of size 238, which is longer than the specified 200\n",
      "Created a chunk of size 292, which is longer than the specified 200\n",
      "Created a chunk of size 698, which is longer than the specified 200\n",
      "Created a chunk of size 456, which is longer than the specified 200\n",
      "Created a chunk of size 599, which is longer than the specified 200\n",
      "Created a chunk of size 394, which is longer than the specified 200\n",
      "Created a chunk of size 1322, which is longer than the specified 200\n",
      "Created a chunk of size 569, which is longer than the specified 200\n",
      "Created a chunk of size 357, which is longer than the specified 200\n",
      "Created a chunk of size 575, which is longer than the specified 200\n",
      "Created a chunk of size 377, which is longer than the specified 200\n",
      "Created a chunk of size 717, which is longer than the specified 200\n",
      "Created a chunk of size 394, which is longer than the specified 200\n",
      "Created a chunk of size 308, which is longer than the specified 200\n",
      "Created a chunk of size 328, which is longer than the specified 200\n",
      "Created a chunk of size 574, which is longer than the specified 200\n",
      "Created a chunk of size 469, which is longer than the specified 200\n",
      "Created a chunk of size 536, which is longer than the specified 200\n",
      "Created a chunk of size 264, which is longer than the specified 200\n",
      "Created a chunk of size 464, which is longer than the specified 200\n",
      "Created a chunk of size 255, which is longer than the specified 200\n",
      "Created a chunk of size 574, which is longer than the specified 200\n",
      "Created a chunk of size 202, which is longer than the specified 200\n",
      "Created a chunk of size 672, which is longer than the specified 200\n",
      "Created a chunk of size 553, which is longer than the specified 200\n",
      "Created a chunk of size 229, which is longer than the specified 200\n",
      "Created a chunk of size 561, which is longer than the specified 200\n",
      "Created a chunk of size 683, which is longer than the specified 200\n",
      "Created a chunk of size 317, which is longer than the specified 200\n",
      "Created a chunk of size 229, which is longer than the specified 200\n",
      "Created a chunk of size 249, which is longer than the specified 200\n",
      "Created a chunk of size 589, which is longer than the specified 200\n",
      "Created a chunk of size 390, which is longer than the specified 200\n",
      "Created a chunk of size 350, which is longer than the specified 200\n",
      "Created a chunk of size 258, which is longer than the specified 200\n",
      "Created a chunk of size 625, which is longer than the specified 200\n",
      "Created a chunk of size 427, which is longer than the specified 200\n",
      "Created a chunk of size 245, which is longer than the specified 200\n",
      "Created a chunk of size 1866, which is longer than the specified 200\n",
      "Created a chunk of size 251, which is longer than the specified 200\n",
      "Created a chunk of size 641, which is longer than the specified 200\n",
      "Created a chunk of size 404, which is longer than the specified 200\n",
      "Created a chunk of size 449, which is longer than the specified 200\n",
      "Created a chunk of size 615, which is longer than the specified 200\n",
      "Created a chunk of size 424, which is longer than the specified 200\n",
      "Created a chunk of size 281, which is longer than the specified 200\n",
      "Created a chunk of size 850, which is longer than the specified 200\n",
      "Created a chunk of size 401, which is longer than the specified 200\n",
      "Created a chunk of size 357, which is longer than the specified 200\n",
      "Created a chunk of size 252, which is longer than the specified 200\n",
      "Created a chunk of size 256, which is longer than the specified 200\n",
      "Created a chunk of size 268, which is longer than the specified 200\n",
      "Created a chunk of size 225, which is longer than the specified 200\n",
      "Created a chunk of size 458, which is longer than the specified 200\n",
      "Created a chunk of size 594, which is longer than the specified 200\n",
      "Created a chunk of size 317, which is longer than the specified 200\n",
      "Created a chunk of size 280, which is longer than the specified 200\n",
      "Created a chunk of size 444, which is longer than the specified 200\n",
      "Created a chunk of size 222, which is longer than the specified 200\n",
      "Created a chunk of size 343, which is longer than the specified 200\n",
      "Created a chunk of size 352, which is longer than the specified 200\n",
      "Created a chunk of size 259, which is longer than the specified 200\n",
      "Created a chunk of size 209, which is longer than the specified 200\n",
      "Created a chunk of size 334, which is longer than the specified 200\n",
      "Created a chunk of size 311, which is longer than the specified 200\n",
      "Created a chunk of size 212, which is longer than the specified 200\n",
      "Created a chunk of size 583, which is longer than the specified 200\n",
      "Created a chunk of size 206, which is longer than the specified 200\n",
      "Created a chunk of size 234, which is longer than the specified 200\n",
      "Created a chunk of size 497, which is longer than the specified 200\n",
      "Created a chunk of size 899, which is longer than the specified 200\n",
      "Created a chunk of size 302, which is longer than the specified 200\n",
      "Created a chunk of size 457, which is longer than the specified 200\n",
      "Created a chunk of size 341, which is longer than the specified 200\n",
      "Created a chunk of size 237, which is longer than the specified 200\n",
      "Created a chunk of size 203, which is longer than the specified 200\n",
      "Created a chunk of size 236, which is longer than the specified 200\n",
      "Created a chunk of size 563, which is longer than the specified 200\n",
      "Created a chunk of size 683, which is longer than the specified 200\n",
      "Created a chunk of size 402, which is longer than the specified 200\n",
      "Created a chunk of size 236, which is longer than the specified 200\n",
      "Created a chunk of size 280, which is longer than the specified 200\n",
      "Created a chunk of size 548, which is longer than the specified 200\n",
      "Created a chunk of size 336, which is longer than the specified 200\n",
      "Created a chunk of size 302, which is longer than the specified 200\n",
      "Created a chunk of size 542, which is longer than the specified 200\n",
      "Created a chunk of size 552, which is longer than the specified 200\n",
      "Created a chunk of size 580, which is longer than the specified 200\n",
      "Created a chunk of size 336, which is longer than the specified 200\n",
      "Created a chunk of size 302, which is longer than the specified 200\n",
      "Created a chunk of size 542, which is longer than the specified 200\n",
      "Created a chunk of size 552, which is longer than the specified 200\n",
      "Created a chunk of size 580, which is longer than the specified 200\n",
      "Created a chunk of size 240, which is longer than the specified 200\n",
      "Created a chunk of size 220, which is longer than the specified 200\n",
      "Created a chunk of size 393, which is longer than the specified 200\n",
      "Created a chunk of size 454, which is longer than the specified 200\n",
      "Created a chunk of size 414, which is longer than the specified 200\n",
      "Created a chunk of size 373, which is longer than the specified 200\n",
      "Created a chunk of size 567, which is longer than the specified 200\n",
      "Created a chunk of size 266, which is longer than the specified 200\n",
      "Created a chunk of size 285, which is longer than the specified 200\n",
      "Created a chunk of size 273, which is longer than the specified 200\n",
      "Created a chunk of size 454, which is longer than the specified 200\n",
      "Created a chunk of size 425, which is longer than the specified 200\n",
      "Created a chunk of size 422, which is longer than the specified 200\n",
      "Created a chunk of size 968, which is longer than the specified 200\n",
      "Created a chunk of size 231, which is longer than the specified 200\n",
      "Created a chunk of size 300, which is longer than the specified 200\n",
      "Created a chunk of size 245, which is longer than the specified 200\n",
      "Created a chunk of size 262, which is longer than the specified 200\n",
      "Created a chunk of size 215, which is longer than the specified 200\n",
      "Created a chunk of size 544, which is longer than the specified 200\n",
      "Created a chunk of size 229, which is longer than the specified 200\n",
      "Created a chunk of size 494, which is longer than the specified 200\n",
      "Created a chunk of size 209, which is longer than the specified 200\n",
      "Created a chunk of size 251, which is longer than the specified 200\n",
      "Created a chunk of size 310, which is longer than the specified 200\n",
      "Created a chunk of size 235, which is longer than the specified 200\n",
      "Created a chunk of size 454, which is longer than the specified 200\n",
      "Created a chunk of size 444, which is longer than the specified 200\n",
      "Created a chunk of size 470, which is longer than the specified 200\n",
      "Created a chunk of size 591, which is longer than the specified 200\n",
      "Created a chunk of size 276, which is longer than the specified 200\n",
      "Created a chunk of size 282, which is longer than the specified 200\n",
      "Created a chunk of size 432, which is longer than the specified 200\n",
      "Created a chunk of size 391, which is longer than the specified 200\n",
      "Created a chunk of size 207, which is longer than the specified 200\n",
      "Created a chunk of size 222, which is longer than the specified 200\n",
      "Created a chunk of size 214, which is longer than the specified 200\n",
      "Created a chunk of size 387, which is longer than the specified 200\n",
      "Created a chunk of size 654, which is longer than the specified 200\n",
      "Created a chunk of size 792, which is longer than the specified 200\n",
      "Created a chunk of size 740, which is longer than the specified 200\n",
      "Created a chunk of size 243, which is longer than the specified 200\n",
      "Created a chunk of size 238, which is longer than the specified 200\n",
      "Created a chunk of size 328, which is longer than the specified 200\n",
      "Created a chunk of size 473, which is longer than the specified 200\n",
      "Created a chunk of size 807, which is longer than the specified 200\n",
      "Created a chunk of size 303, which is longer than the specified 200\n",
      "Created a chunk of size 429, which is longer than the specified 200\n",
      "Created a chunk of size 472, which is longer than the specified 200\n",
      "Created a chunk of size 233, which is longer than the specified 200\n",
      "Created a chunk of size 382, which is longer than the specified 200\n",
      "Created a chunk of size 745, which is longer than the specified 200\n",
      "Created a chunk of size 895, which is longer than the specified 200\n",
      "Created a chunk of size 581, which is longer than the specified 200\n",
      "Created a chunk of size 1043, which is longer than the specified 200\n",
      "Created a chunk of size 389, which is longer than the specified 200\n",
      "Created a chunk of size 218, which is longer than the specified 200\n",
      "Created a chunk of size 244, which is longer than the specified 200\n",
      "Created a chunk of size 499, which is longer than the specified 200\n",
      "/var/folders/s0/q09ljrf52bjgnb563vkhs4rw0000gn/T/ipykernel_46432/842990402.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "/var/folders/s0/q09ljrf52bjgnb563vkhs4rw0000gn/T/ipykernel_46432/842990402.py:8: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "# Setup Local Document Retriever for Hybrid RAG\n",
    "# This creates the vector-based retrieval component of the hybrid system\n",
    "def get_reteriver() : \n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load UCSC knowledge base from CSV\n",
    "    # This forms the foundation of our local retrieval capability\n",
    "    ucsc_passage_df = pd.read_csv(\"passage.csv\")\n",
    "    \n",
    "    # Convert to LangChain document format\n",
    "    # Each row becomes a searchable document\n",
    "    ucsc_passge_data_loader = DataFrameLoader(ucsc_passage_df, page_content_column=\"passage\")\n",
    "    ucsc_passage_data = ucsc_passge_data_loader.load()\n",
    "    \n",
    "    # Chunk documents for optimal retrieval\n",
    "    # Small chunks with overlap ensure comprehensive coverage\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(ucsc_passage_data)\n",
    "    \n",
    "    # Create semantic embeddings for vector search\n",
    "    # Enables similarity-based document retrieval\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    \n",
    "    # Build FAISS vector database\n",
    "    # Provides fast similarity search capabilities\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    \n",
    "    # Return retriever interface for the hybrid system\n",
    "    return db.as_retriever()\n",
    "\n",
    "# Initialize the local retriever\n",
    "# This will be one component of our multi-source retrieval strategy\n",
    "retriever = get_reteriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c615fcd6-ac86-46b1-be2d-dd532c27c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Web Search Component for Hybrid RAG\n",
    "# This enables the system to search beyond local knowledge when needed\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain.schema import HumanMessage\n",
    "from typing import List, TypedDict\n",
    "import re\n",
    "\n",
    "# Initialize Tavily search with advanced configuration\n",
    "# This provides the web search capability for our hybrid approach\n",
    "tavily = TavilySearch(\n",
    "    max_results=5,          # Limit results for focused information\n",
    "    search_depth=\"advanced\", # Use advanced search for better quality\n",
    "    include_answer=True,     # Include direct answers when available\n",
    "    include_raw_content=True, # Include full content for context\n",
    "    api_key=tavily_api_key   # Use API key from environment\n",
    ")\n",
    "\n",
    "# Import message types for LLM interactions\n",
    "# These are used throughout the hybrid system for structured communication\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b702892-7ca5-4272-ba46-2702eda74371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYBRID RAG HELPER FUNCTIONS ===\n",
    "# This comprehensive collection of functions powers the meta-RAG system\n",
    "# Each function handles a specific aspect of the intelligent retrieval workflow\n",
    "\n",
    "# Initialize the core LLM for all operations\n",
    "# Temperature=0.0 ensures consistent, deterministic behavior\n",
    "llm = ChatOpenAI(temperature=0.0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# === RETRIEVAL FUNCTIONS ===\n",
    "\n",
    "def vector_search(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Perform vector-based similarity search on local knowledge base\n",
    "    Returns: List of relevant document contents as strings\n",
    "    \"\"\"\n",
    "    docs: List[Document] = retriever.get_relevant_documents(query)\n",
    "    # Extract only the text content from document objects\n",
    "    return [d.page_content for d in docs]\n",
    "    \n",
    "\n",
    "def web_search(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Perform web search using Tavily API\n",
    "    Returns: List of web search results as strings\n",
    "    \"\"\"\n",
    "    return tavily.invoke(query)  \n",
    "\n",
    "# === EVALUATION AND SCORING FUNCTIONS ===\n",
    "\n",
    "def score_docs(docs: List[str], query: str) -> float:\n",
    "    \"\"\"\n",
    "    Use LLM to rate relevance of documents to UCSC-specific query\n",
    "    Returns: Confidence score from 0.0 (irrelevant) to 1.0 (highly relevant)\n",
    "    \n",
    "    This is crucial for Hybrid RAG's adaptive behavior - it determines\n",
    "    whether to continue with current results or try alternative strategies\n",
    "    \"\"\"\n",
    "    # Ensure UCSC context is explicit in the evaluation\n",
    "    refined_query = f\"{query} (UCSC = University of California Santa Cruz)\"\n",
    "\n",
    "    # Format documents as numbered list for LLM evaluation\n",
    "    doc_list = \"\\n\".join(f\"{i+1}. {d}\" for i, d in enumerate(docs))\n",
    "    prompt = (\n",
    "        \"You are evaluating how well each document helps answer a question about UCSC \"\n",
    "        \"(University of California Santa Cruz). \"\n",
    "        \"Rate relevance from 0.0 (irrelevant) to 1.0 (fully relevant).\\n\\n\"\n",
    "        f\"Query: {refined_query}\\n\\n\"\n",
    "        f\"Documents:\\n{doc_list}\\n\\n\"\n",
    "        \"Respond with ONLY a single number between 0.0 and 1.0.\"\n",
    "    )\n",
    "\n",
    "    # Get LLM evaluation\n",
    "    resp = llm.invoke([HumanMessage(content=prompt)])\n",
    "    text = resp.content.strip()\n",
    "\n",
    "    # Parse score and ensure it's within valid range\n",
    "    try:\n",
    "        score = float(text)\n",
    "        return max(0.0, min(1.0, score))\n",
    "    except ValueError:\n",
    "        return 0.0  # Default to no confidence if parsing fails\n",
    "\n",
    "# === QUERY OPTIMIZATION FUNCTIONS ===\n",
    "\n",
    "def rewrite_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Rewrite query for optimal vector retrieval against UCSC knowledge base\n",
    "    \n",
    "    This function enhances queries with:\n",
    "    - Explicit UCSC context\n",
    "    - Academic keywords\n",
    "    - Domain-specific terminology\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You're rewriting a student's question into a concise vector-search query \"\n",
    "        \"targeting UCSC documents.  \\n\"\n",
    "        \"- Always include \"UCSC (University of California Santa Cruz)\".\\n\"\n",
    "        \"- Add academic keywords like course codes, department names, document types.\\n\"\n",
    "        \"- Keep it to 3–6 key terms or short phrases.\\n\\n\"\n",
    "        f\"Original question: {query}\\n\\n\"\n",
    "        \"Vector-search query:\"\n",
    "    )\n",
    "    resp = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return resp.content.strip()\n",
    "\n",
    "def rewrite_web_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Rewrite query for optimal web search results\n",
    "    \n",
    "    This function optimizes queries for:\n",
    "    - Web search engines\n",
    "    - Site-specific searches\n",
    "    - Online forum searches\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You're converting a student's question into an advanced web search query \"\n",
    "        \"about UCSC (University of California Santa Cruz).  \\n\"\n",
    "        \"- Mention \"UCSC (University of California Santa Cruz)\" explicitly.\\n\"\n",
    "        \"- Favor site-specific qualifiers (e.g., site:ucsc.edu) or student/departmental forums.\\n\"\n",
    "        \"- Keep it concise—aim for a 5–8 word search string.\\n\\n\"\n",
    "        f\"Original question: {query}\\n\\n\"\n",
    "        \"Web search query:\"\n",
    "    )\n",
    "    resp = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return resp.content.strip()\n",
    "\n",
    "# === ANSWER SYNTHESIS FUNCTIONS ===\n",
    "\n",
    "def synthesize_answer(context: str, query: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate final answer by combining retrieved context with original query\n",
    "    \n",
    "    Uses strict guidelines to:\n",
    "    - Prevent hallucination\n",
    "    - Ensure grounding in provided context\n",
    "    - Maintain answer quality\n",
    "    \"\"\"\n",
    "    system_msg = SystemMessage(content=(\n",
    "    \"You are a UCSC assistant. Only use the provided context and the original query to answer. \"\n",
    "    \"Do NOT hallucinate or invent facts. If you're not 100% sure, say \"I'm sorry, I don't know.\"\"\n",
    "))\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"Original Query: {query}\\n\\n\"\n",
    "        f\"Context from UCSC sources:\\n{context}\\n\\n\"\n",
    "        \"Based on this information and the query above, \"\n",
    "        \"provide a clear and direct answer. \"\n",
    "        \"If information is partial, focus on what you DO know rather than what you don't.\"\n",
    "    )\n",
    "    resp = llm.invoke([system_msg, HumanMessage(content=user_prompt)])\n",
    "    return resp.content.strip()\n",
    "\n",
    "# === CHAIN-OF-THOUGHT FUNCTIONS ===\n",
    "\n",
    "import json\n",
    "\n",
    "def break_into_cot_questions( query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Decompose complex query into logical sub-questions\n",
    "    \n",
    "    This enables the CoT (Chain-of-Thought) fallback strategy\n",
    "    when direct retrieval doesn't yield sufficient confidence\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a UCSC (University of California Santa Cruz) assistant.  \\n\"\n",
    "        \"Break down this query into 3–5 logical subquestions.  \\n\"\n",
    "        \"Output MUST be valid JSON, a top-level array of strings.  \\n\\n\"\n",
    "        f\"Query: {query}\\n\\n\"\n",
    "        \"Example output:\\n\"\n",
    "        '[\"What is …?\", \"How does …?\"]\\n\\n'\n",
    "        \"Now your output:\"\n",
    "    )\n",
    "    resp = llm.invoke([HumanMessage(content=prompt)])\n",
    "    content = resp.content.strip()\n",
    "\n",
    "    try:\n",
    "        questions = json.loads(content)\n",
    "        if isinstance(questions, list) and all(isinstance(q, str) for q in questions):\n",
    "            return questions\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Fallback parsing if JSON fails\n",
    "    return [q.strip() for q in content.split(\"\\n\") if q.strip()]\n",
    "\n",
    "# === QUALITY ASSESSMENT FUNCTIONS ===\n",
    "\n",
    "def score_answer(answer: str, query: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate answer quality and completeness\n",
    "    \n",
    "    Returns confidence score (0.0-1.0) indicating how well\n",
    "    the answer addresses the original query\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are grading the quality of an answer to a UCSC (University of California Santa Cruz) question.\\n\\n\"\n",
    "        f\"Query: {query}\\n\\n\"\n",
    "        f\"Answer: {answer}\\n\\n\"\n",
    "        \"On a scale from 0.0 (not at all confident) to 1.0 (extremely confident), \"\n",
    "        \"how well does this answer address the query accurately and completely? \"\n",
    "        \"Respond with ONLY a single number.\"\n",
    "    )\n",
    "    resp = llm.invoke([HumanMessage(content=prompt)])\n",
    "    text = resp.content.strip()\n",
    "    try:\n",
    "        score = float(text)\n",
    "        return max(0.0, min(1.0, score))\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "def rerank_docs(docs: List[str], query: str, top_k: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Use LLM to intelligently rerank and select most relevant documents\n",
    "    \n",
    "    This provides more sophisticated document selection than\n",
    "    simple similarity scores alone\n",
    "    \"\"\"\n",
    "    numbered = \"\\n\".join(f\"{i+1}. {d}\" for i, d in enumerate(docs))\n",
    "    prompt = (\n",
    "        f\"You are selecting the {top_k} most relevant UCSC (University of California Santa Cruz) documents \"\n",
    "        f\"for the query: {query}\\n\\n\"\n",
    "        f\"Documents:\\n{numbered}\\n\\n\"\n",
    "        \"Respond with a JSON list of the 1-based indices of the top documents, e.g. [1,3,5].\"\n",
    "    )\n",
    "    resp = llm.invoke([HumanMessage(content=prompt)])\n",
    "    try:\n",
    "        inds = json.loads(resp.content.strip())\n",
    "        return [docs[i-1] for i in inds if 1 <= i <= len(docs)]\n",
    "    except Exception:\n",
    "        return docs[:top_k]  # Fallback to first k documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd3133-7316-4393-b2d1-99f6694d02a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LANGGRAPH-BASED HYBRID RAG AGENT ===\n",
    "# This implementation uses LangGraph to create a stateful workflow\n",
    "# that manages the complex decision-making process of the hybrid system\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# === STATE MANAGEMENT ===\n",
    "# Define the state schema that tracks information through the workflow\n",
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    State object that persists information through the agent workflow\n",
    "    \n",
    "    Attributes:\n",
    "        query: Original user question\n",
    "        cot_questions: Sub-questions from chain-of-thought decomposition  \n",
    "        docs: Retrieved documents used for answer generation\n",
    "        answer: Final generated answer\n",
    "        direct: Strategy indicator (1=direct, 2=CoT, 3=failed)\n",
    "    \"\"\"\n",
    "    query: str\n",
    "    cot_questions: List[str]\n",
    "    docs: List[str]\n",
    "    answer: str\n",
    "    direct: int  \n",
    "\n",
    "# === CORE RETRIEVAL TOOL ===\n",
    "def retriever_tool(query: str, use_web: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Unified retrieval function that combines vector search with web fallback\n",
    "    \n",
    "    This tool implements the adaptive retrieval strategy:\n",
    "    1. Start with vector search on local knowledge base\n",
    "    2. Iteratively rewrite query if confidence is low\n",
    "    3. Fall back to web search if local results insufficient\n",
    "    4. Return best available results as concatenated string\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        use_web: Whether to allow web search fallback\n",
    "        \n",
    "    Returns:\n",
    "        Combined document text separated by newlines\n",
    "    \"\"\"\n",
    "    # === PHASE 1: VECTOR SEARCH WITH QUERY OPTIMIZATION ===\n",
    "    docs = vector_search(query)\n",
    "    score = score_docs(docs, query)\n",
    "    retries = 0\n",
    "    \n",
    "    # Iteratively improve query if confidence is low\n",
    "    while score < 0.6 and retries < 3:\n",
    "        query = rewrite_query(query)  # Optimize for vector retrieval\n",
    "        docs = vector_search(query)\n",
    "        score = score_docs(docs, query)\n",
    "        retries += 1\n",
    "    \n",
    "    # === PHASE 2: WEB SEARCH FALLBACK (if enabled and needed) ===\n",
    "    if use_web and score < 0.6:\n",
    "        web_docs = web_search(query)\n",
    "        web_score = score_docs(web_docs, query)\n",
    "        web_retries = 0\n",
    "        \n",
    "        # Optimize web query if results are poor\n",
    "        while web_score < 0.6 and web_retries < 3:\n",
    "            query = rewrite_web_query(query)  # Optimize for web search\n",
    "            web_docs = web_search(query)\n",
    "            web_score = score_docs(web_docs, query)\n",
    "            web_retries += 1\n",
    "        \n",
    "        # Use web results if they're better than vector results\n",
    "        docs = web_docs if web_score > score else docs\n",
    "    \n",
    "    return \"\\n\".join(docs)\n",
    "\n",
    "# === MAIN AGENT NODE ===\n",
    "def agent_node(state: State) -> State:\n",
    "    \"\"\"\n",
    "    Main agent workflow that implements the complete Hybrid RAG strategy\n",
    "    \n",
    "    Workflow:\n",
    "    1. Try direct retrieval with high confidence threshold\n",
    "    2. If unsuccessful, decompose query using Chain-of-Thought\n",
    "    3. Process each sub-question individually with web search\n",
    "    4. Synthesize final answer from combined results\n",
    "    5. Return appropriate state based on success/failure\n",
    "    \"\"\"\n",
    "    \n",
    "    # === STRATEGY 1: DIRECT RETRIEVAL ===\n",
    "    print(\"Attempting direct retrieval...\")\n",
    "    \n",
    "    # Get initial results using only local knowledge (no web fallback)\n",
    "    raw = retriever_tool(state[\"query\"], use_web=False)\n",
    "    initial_docs = raw.split(\"\\n\")\n",
    "\n",
    "    # Generate answer and evaluate confidence\n",
    "    initial_answer = synthesize_answer(initial_docs, state[\"query\"])\n",
    "    conf_initial = score_answer(initial_answer, state[\"query\"])\n",
    "\n",
    "    # If direct retrieval is confident enough, use it\n",
    "    if conf_initial >= 0.6:\n",
    "        print(f\"Direct retrieval succeeded with confidence {conf_initial:.2f}\")\n",
    "        return {**state, \"docs\": initial_docs, \"answer\": initial_answer, \"direct\": 1}\n",
    "\n",
    "    # === STRATEGY 2: CHAIN-OF-THOUGHT DECOMPOSITION ===\n",
    "    print(\"Direct retrieval insufficient, trying Chain-of-Thought...\")\n",
    "    \n",
    "    # Break complex query into simpler sub-questions\n",
    "    qs = break_into_cot_questions(state[\"query\"])\n",
    "    all_docs: List[str] = []\n",
    "    \n",
    "    # Process each sub-question individually\n",
    "    for i, q in enumerate(qs):\n",
    "        print(f\"  Processing sub-question {i+1}/{len(qs)}: {q}\")\n",
    "        \n",
    "        # Use full retrieval pipeline (including web search) for sub-questions\n",
    "        part = retriever_tool(q, use_web=True)   \n",
    "        chunk_docs = part.split(\"\\n\")\n",
    "        \n",
    "        # Intelligently select best documents for this sub-question\n",
    "        chunk_docs = rerank_docs(chunk_docs, q, top_k=5)\n",
    "        all_docs.extend(chunk_docs)                  \n",
    "\n",
    "    # === FINAL SYNTHESIS ===\n",
    "    # Combine all retrieved documents into comprehensive context\n",
    "    context = \"\\n\".join(all_docs)\n",
    "    final_answer = synthesize_answer(context, state[\"query\"])\n",
    "    conf_final = score_answer(final_answer, state[\"query\"])\n",
    "\n",
    "    # Check if final answer meets minimum confidence threshold\n",
    "    if conf_final < 0.6:\n",
    "        print(\"All strategies failed to meet confidence threshold\")\n",
    "        return {**state, \"docs\": [], \"answer\": \"NO_ANSWER\", \"direct\": 3}\n",
    "\n",
    "    print(f\"Chain-of-Thought succeeded with confidence {conf_final:.2f}\")\n",
    "    return {**state, \"docs\": all_docs, \"answer\": final_answer, \"direct\": 2}\n",
    "\n",
    "# === GRAPH CONSTRUCTION ===\n",
    "# Build the LangGraph workflow with our agent node\n",
    "print(\"Building Hybrid RAG workflow graph...\")\n",
    "\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f583c6dc-a59a-48ef-9d8b-a29feb4406ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/var/folders/s0/q09ljrf52bjgnb563vkhs4rw0000gn/T/ipykernel_46432/2640827723.py:5: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs: List[Document] = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCSC Extension Silicon Valley Campus provides UC-approved courses, professional certificate programs, skills-based intensives, and boot camps to students and professionals at every stage of their careers.\n"
     ]
    }
   ],
   "source": [
    "# === COMPILE AND TEST THE HYBRID RAG GRAPH ===\n",
    "# Compile the LangGraph workflow into an executable application\n",
    "app = graph.compile()\n",
    "\n",
    "# Test the system with a sample UCSC-specific query\n",
    "print(\"Testing Hybrid RAG with sample query...\")\n",
    "\n",
    "init_state = {\n",
    "    \"query\": \"What classes does UCSC Extension Silicon Valley Campus provide?\",\n",
    "    \"cot_questions\": [],  # Will be populated if CoT strategy is used\n",
    "    \"docs\": [],           # Will contain retrieved documents  \n",
    "    \"answer\": \"\"          # Will contain final generated answer\n",
    "}\n",
    "\n",
    "# Execute the workflow and get results\n",
    "result = app.invoke(init_state)\n",
    "print(\"Final Answer:\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68610576-d175-4d00-88a4-04f827a90437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which strategy was used to generate the answer\n",
    "# 1 = Direct retrieval succeeded\n",
    "# 2 = Chain-of-Thought strategy succeeded  \n",
    "# 3 = All strategies failed\n",
    "print(f\"Strategy used: {result['direct']}\")\n",
    "result[\"direct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ccb62-3c33-42fd-a43e-88df6afec058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPREHENSIVE EVALUATION SETUP ===\n",
    "# Set up RAGAS (RAG Assessment) framework for systematic evaluation\n",
    "# This provides standardized metrics for RAG system performance\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
    "\n",
    "# === EVALUATION DATA STORAGE ===\n",
    "# Lists to collect evaluation data for batch processing\n",
    "question_list = []   # User queries\n",
    "gen_ans_list  = []   # Generated answers from Hybrid RAG\n",
    "searched_docs = []   # Retrieved documents used for answers\n",
    "src_ans_list  = []   # Ground truth answers (if available)\n",
    "\n",
    "def run_query(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Execute a single query through the Hybrid RAG system\n",
    "    \n",
    "    This wrapper function:\n",
    "    1. Initializes the state for the query\n",
    "    2. Executes the complete workflow\n",
    "    3. Returns detailed results including strategy used\n",
    "    \n",
    "    Args:\n",
    "        query: User question to process\n",
    "        \n",
    "    Returns:\n",
    "        Complete result dictionary with answer, docs, and metadata\n",
    "    \"\"\"\n",
    "    init_state = {\n",
    "        \"query\":         query,\n",
    "        \"cot_questions\": [],  # Will be populated during CoT if needed\n",
    "        \"docs\":          [],  # Will contain retrieved context documents\n",
    "        \"answer\":        \"\",  # Final generated answer\n",
    "        \"direct\":        0    # Strategy indicator (1=direct, 2=CoT, 3=failed)\n",
    "    }\n",
    "    return app.invoke(init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bd124-f4d9-4e44-a3e4-2c2091341097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing #1: Who can be a member of the Boating Club?\n",
      "Processing #2: What classes does the boating center offer to community members?\n",
      "Processing #3: When is the boating club open?\n",
      "Processing #4: What boats are available to members?\n",
      "Processing #5: How much does a boating membership cost?\n",
      "Processing #6: How do I sign up for a boating membership?\n",
      "Processing #7: How can I join the Boating Club?\n",
      "Processing #8: Are there additional fees to take out the boats after I join the boating club?\n",
      "Processing #9: Do the boating club rent Kayaks?\n",
      "Processing #10: How can I get involved with the Boating Club?\n",
      "Processing #11: Why is health insurance required at UCSC?\n",
      "Processing #12: How do I get UC SHIP?\n",
      "Processing #13: How do I get my UC SHIP ID card?\n",
      "Processing #14: Am I eligible if I opted out of UC SHIP last year?\n",
      "Processing #15: Am I eligible for UC SHIP if I've graduated? \n",
      "Processing #16: How much does UC SHIP cost?\n",
      "Processing #17: Will financial aid pay for my UC SHIP?\n",
      "Processing #18: Will Medi-Cal pay for my UC SHIP?\n",
      "Processing #19: Can I do research at Baskin Engineering?\n",
      "Processing #20: Is there going to be time for extracurricular activities?\n",
      "Processing #21: How hard is it to get into undergrad classes?\n",
      "Processing #22: What are the class sizes for the lower and upper division classes under the engineering school?\n",
      "Processing #23: What is the diversity breakdown for engineering?\n",
      "Processing #24: Is it hard to get help and talk to professors due to large class sizes? \n",
      "Processing #25: Which residential college is best for incoming engineering students?\n",
      "Processing #26: What majors are housed in the school of engineering?\n",
      "Processing #27: Can I switch into computer science if I was accepted into another engineering program?\n",
      "Processing #28: What relationship does Baskin Engineering have with tech in Silicon Valley?\n",
      "Processing #29: What companies collaborate with the engineering school?\n",
      "Processing #30: What internship opportunities are available to students?\n",
      "Processing #31: What are the employment rates of new graduates?\n",
      "Processing #32: Does UCSC offer a ____ major? How is the ___ major at UC Santa Cruz?\n",
      "Processing #33: Does UCSC offer a pre-med program? A business program? What about preparation to become a teacher?\n",
      "Processing #34: May I apply to UCSC with an undeclared major? When must I declare a major?\n",
      "Processing #35: How do you use alternate majors from the UC Application?\n",
      "Processing #36: Can I double major?\n",
      "Processing #37: How large are classes?\n",
      "Processing #38: What are the graduation requirements?\n",
      "Processing #39: What opportunities are there for early graduation?\n",
      "Processing #40: What support do first-year students have? Do they have advisers?\n",
      "Processing #41: What honors courses or programs are available?\n",
      "Processing #42: How can I get a UC Santa Cruz catalog?\n",
      "Processing #43: What is UC Santa Cruz's grading policy for undergraduates?\n",
      "Processing #44: What classes does UCSC Extension Silicon Valley provide?\n",
      "Processing #45: How are you making your admissions decisions?\n",
      "Processing #46: Did you admit any out-of-state or international students?\n",
      "Processing #47: Does UCSC have a waitlist?\n",
      "Processing #48: If I was not selected for admission, may I appeal the decision?\n",
      "Processing #49: What is Dual Admission?\n",
      "Processing #50: How do you make your admissions decisions?\n",
      "Processing #51: Is choice of major one of your criteria for admission/selection?\n",
      "Processing #52: I want to change the major that I listed on my application. Is that possible?\n",
      "Processing #53: Do you take fall term grades into account?\n",
      "Processing #54: Do you give any preference for local students?\n",
      "Processing #55: How are you able to let some students know their decision before other students?\n",
      "Processing #56: Do you take any out-of-state or international students?\n",
      "Processing #57: I think you made a mistake and I'd like to appeal. Do you have an appeals process?\n",
      "Processing #58: I don't want to appeal because I think a mistake has been made. Can you tell me how I can get your decision changed?\n",
      "Processing #59: Have you set aside a specific number of admission spaces for appeal cases?\n",
      "Processing #60: When will I know a decision on my appeal?\n",
      "Processing #61: If my appeal for fall admission is denied, what are my chances for winter admission?\n",
      "Processing #62: Does UCSC have a waitlist?\n",
      "Processing #63: Can I apply for spring quarter?\n",
      "Processing #64: What is the waitlist?\n",
      "Processing #65: How will I know that I've been offered a place on the UCSC waitlist?\n",
      "Processing #66: I had good grades and took a broad range of challenging courses at my prior high school or community college. Why wasn’t I admitted?\n",
      "Processing #67: Am I on the waitlist automatically?\n",
      "Processing #68: If I am on the waitlist for UCSC, what are my chances of being admitted?\n",
      "Processing #69: What number am I on the waitlist?\n",
      "Processing #70: How will I find out if I get an offer of admission?\n",
      "Processing #71: What if I already accepted an offer to another UC campus?\n",
      "Processing #72: Can I be on a waitlist for more than one UC campus?\n",
      "Processing #73: What if I am on the waitlist for UCSC, and I receive an offer from another university outside the University of California system?\n",
      "Processing #74: If I’m admitted from the waitlist, will I still be eligible for financial aid?\n",
      "Processing #75: If I’m admitted from the waitlist, will I get my first-choice residential college?\n",
      "Processing #76: Can I change the college I was assigned to?\n",
      "Processing #77: How do I apply for admission for fall or winter quarters?\n",
      "Processing #78: What standardized tests do I need to take?\n",
      "Processing #79: What is the acceptance rate?\n",
      "Processing #80: Do students from the East Coast have a greater chance of being accepted than West Coast students?\n",
      "Processing #81: What is your AP policy?\n",
      "Processing #82: When are acceptance notices sent out?\n",
      "Processing #83: What is the application and admission process for student athletes?\n",
      "Processing #84: What kinds of athletics and intramural programs are there?\n",
      "Processing #85: Does UC Santa Cruz offer any athletics scholarships or financial aid?\n",
      "Processing #86: What is the level of competition in UC Santa Cruz athletics?\n",
      "Processing #87: How difficult is it to make an NCAA Division III team?\n",
      "Processing #88: How are the athletics facilities at UC Santa Cruz?\n",
      "Processing #89: Does UC Santa Cruz have a housing guarantee?\n",
      "Processing #90: What is the college system?\n",
      "Processing #91: How do I request affiliation with a college and can I change it?\n",
      "Processing #92: Does my college affiliation affect what classes I can take?\n",
      "Processing #93: What services are in place to help me find off-campus housing?\n",
      "Processing #94: How do I apply for Family Student Housing?\n",
      "Processing #95: What types of financial aid are available?\n",
      "Processing #96: What is the Blue and Gold Opportunity Plan?\n",
      "Processing #97: What is the Middle Class Scholarship?\n",
      "Processing #98: What other financing options are available?\n",
      "Processing #99: How do I apply for financial aid?\n",
      "Processing #100: How much financial aid is offered for out-of-state students?\n",
      "Processing #101: How much financial aid is offered for international students?\n",
      "Processing #102: What if I don't qualify for financial aid?\n",
      "Processing #103: What student clubs and organizations are active at Santa Cruz?\n",
      "Processing #104: What kinds of arts and entertainment does the campus offer?\n",
      "Processing #105: Are there any selective majors at UC Santa Cruz?\n",
      "Processing #106: What kinds of courses can I transfer?\n",
      "Processing #107: What is the maximum number of credits I can transfer into UC Santa Cruz with?\n",
      "Processing #108: What if I don't fulfill the general education requirements before I transfer?\n",
      "Processing #109: What is UC TAP?\n",
      "Processing #110: When are acceptance notices sent out?\n",
      "Processing #111: What are the Cross-Campus and Simultaneous Enrollment programs?\n",
      "Processing #112: Can I talk to an adviser during a campus visit?\n"
     ]
    }
   ],
   "source": [
    "# === BATCH EVALUATION PROCESS ===\n",
    "# Process the complete evaluation dataset through Hybrid RAG\n",
    "# This systematically tests the system on multiple queries\n",
    "\n",
    "print(\"Starting batch evaluation of Hybrid RAG system...\")\n",
    "\n",
    "# Load evaluation dataset with questions and ground truth answers\n",
    "df = pd.read_csv(\"new_qa.csv\")  # expects columns \"questions\" and \"answers\"\n",
    "\n",
    "# Process each question in the dataset\n",
    "for idx, row in df.iterrows():\n",
    "    q = row[\"questions\"]         # User query\n",
    "    a = row[\"answers\"]           # Ground truth answer\n",
    "    print(f\"Processing #{idx+1}: {q}\")\n",
    "\n",
    "    # Always record the ground truth answer for later comparison\n",
    "    src_ans_list.append(a)\n",
    "\n",
    "    # Execute the query through our Hybrid RAG system\n",
    "    final_state = run_query(q)\n",
    "\n",
    "    # Skip questions that couldn't be answered\n",
    "    # This maintains alignment between successful answers and ground truth\n",
    "    if final_state[\"answer\"] == \"NO_ANSWER\":\n",
    "        print(f\" No answer generated for query #{idx+1}\")\n",
    "        continue\n",
    "\n",
    "    # Collect successful results for evaluation\n",
    "    question_list.append(q)\n",
    "    gen_ans_list.append(final_state[\"answer\"])\n",
    "    searched_docs.append(final_state[\"docs\"])\n",
    "    print(f\" Answer generated using strategy {final_state['direct']}\")\n",
    "\n",
    "print(f\"\\n Evaluation complete: {len(gen_ans_list)} answers generated from {len(df)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b149d27f-2471-46f9-ac99-b0a1c9b06825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What classes does UCSC Extension Silicon Valley provide?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect a specific question from the dataset for analysis\n",
    "# This helps understand the types of queries being evaluated\n",
    "df = pd.read_csv(\"new_qa.csv\")\n",
    "print(\"Sample question #44:\")\n",
    "df[\"questions\"][43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a086ada-7dbd-48a8-b6d8-abf069ad4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXPORT EVALUATION RESULTS ===\n",
    "# Save the generated outputs for further analysis and comparison\n",
    "# This creates a permanent record of system performance\n",
    "\n",
    "print(\"Exporting evaluation results...\")\n",
    "\n",
    "df_out = pd.DataFrame({\n",
    "    \"question\":         question_list,    # Queries that received answers\n",
    "    \"generated_answer\": gen_ans_list,     # Hybrid RAG responses\n",
    "    \"supporting_docs\":  searched_docs     # Documents used for each answer\n",
    "})\n",
    "\n",
    "# Export to CSV for analysis and comparison with other RAG approaches\n",
    "df_out.to_csv(\"HybridRAG_generated_outputsFinal.csv\", index=False)\n",
    "print(f\"Exported {len(df_out)} results to HybridRAG_generated_outputsFinal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda8c00-3114-4658-b1c4-4aacd7c03c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 107, 107, 112)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify data consistency before RAGAS evaluation\n",
    "# All lists should have the same length for successful questions\n",
    "print(\"Data consistency check:\")\n",
    "print(f\"Generated answers: {len(gen_ans_list)}\")\n",
    "print(f\"Questions: {len(question_list)}\")\n",
    "print(f\"Supporting docs: {len(searched_docs)}\")\n",
    "print(f\"Ground truth answers: {len(src_ans_list)}\")\n",
    "\n",
    "len(gen_ans_list), len(question_list), len(searched_docs), len(src_ans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a33d0-6fb2-4b5e-8380-3483d857e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ALIGN GROUND TRUTH WITH GENERATED ANSWERS ===\n",
    "# Since we skipped some questions (NO_ANSWER cases), we need to \n",
    "# align ground truth answers with only the questions we actually answered\n",
    "\n",
    "print(\"Aligning ground truth answers with successful generations...\")\n",
    "\n",
    "# Create mapping from questions to ground truth answers\n",
    "gt_map = dict(zip(df[\"questions\"], df[\"answers\"]))\n",
    "\n",
    "# Filter ground truth to match only the questions we answered\n",
    "# This ensures proper alignment for RAGAS evaluation\n",
    "src_ans_list = [gt_map[q] for q in question_list]\n",
    "\n",
    "print(f\"Aligned {len(src_ans_list)} ground truth answers with generated answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1121e-6957-4ae5-95fd-22034b8c35ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 107, 107, 107)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final verification: all data should now be properly aligned\n",
    "# This confirms we can proceed with RAGAS evaluation\n",
    "print(\"Final alignment verification:\")\n",
    "print(f\"All lists should be equal: {len(gen_ans_list)} = {len(question_list)} = {len(searched_docs)} = {len(src_ans_list)}\")\n",
    "\n",
    "len(gen_ans_list), len(question_list), len(searched_docs), len(src_ans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08086544-5c71-49e2-8435-444378a496da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE RAGAS EVALUATION DATASET ===\n",
    "# Prepare data in the format required by RAGAS framework\n",
    "# RAGAS expects specific field names for automated evaluation\n",
    "\n",
    "print(\"Creating RAGAS evaluation dataset...\")\n",
    "\n",
    "ragas_dataset = Dataset.from_dict({\n",
    "    \"question\":           question_list,    # User queries\n",
    "    \"ground_truth\":       src_ans_list,     # Expected correct answers\n",
    "    \"answer\":             gen_ans_list,     # Hybrid RAG generated answers\n",
    "    \"retrieved_contexts\": searched_docs     # Supporting documents used\n",
    "})\n",
    "\n",
    "print(f\"RAGAS dataset created with {len(question_list)} examples\")\n",
    "print(\"Dataset ready for comprehensive evaluation with multiple metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4ac44-ff4d-47fa-8c70-8b6e8813839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 428/428 [05:22<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            user_input  \\\n",
      "0             Who can be a member of the Boating Club?   \n",
      "1    What classes does the boating center offer to ...   \n",
      "2                       When is the boating club open?   \n",
      "3                 What boats are available to members?   \n",
      "4             How much does a boating membership cost?   \n",
      "..                                                 ...   \n",
      "102  What if I don't fulfill the general education ...   \n",
      "103                                    What is UC TAP?   \n",
      "104              When are acceptance notices sent out?   \n",
      "105  What are the Cross-Campus and Simultaneous Enr...   \n",
      "106    Can I talk to an adviser during a campus visit?   \n",
      "\n",
      "                                    retrieved_contexts  \\\n",
      "0    [How can I join the Boating Club?, The boating...   \n",
      "1    [UC Santa Cruz Community Boating Center is a c...   \n",
      "2    [Boating Club Hours, The Community Boating Cen...   \n",
      "3    [Our boats:, Sailing vessels for weekend use c...   \n",
      "4    [Cost, Club Membership Fees:, , UCSC Student M...   \n",
      "..                                                 ...   \n",
      "102  [By the end of the fall term prior to transfer...   \n",
      "103  [UCSC TPP, UCSC TAG Step-by-Step, Complete the...   \n",
      "104  [For fall quarter acceptance, most notices are...   \n",
      "105  [During fall, winter, and spring quarters, und...   \n",
      "106  [College advisors and preceptors are “generali...   \n",
      "\n",
      "                                              response  \\\n",
      "0    Members of the Boating Club can include commun...   \n",
      "1    The UC Santa Cruz Community Boating Center off...   \n",
      "2    The Boating Club is open on Saturdays and Sund...   \n",
      "3    The boats available to members include RS Ques...   \n",
      "4    The cost of a boating membership at UCSC is $4...   \n",
      "..                                                 ...   \n",
      "102  If you don't fulfill the general education req...   \n",
      "103  UC TAP stands for UC Transfer Admission Planne...   \n",
      "104  Acceptance notices for fall quarter are sent o...   \n",
      "105  The Cross-Campus and Simultaneous Enrollment p...   \n",
      "106  Yes, you can talk to an adviser during a campu...   \n",
      "\n",
      "                                             reference  faithfulness  \\\n",
      "0    Everyone is welcome to join! If you do not hav...      1.000000   \n",
      "1    We offer all levels of sailing (dinghy and kee...      0.714286   \n",
      "2    The club is open year-round, Saturday and Sund...      1.000000   \n",
      "3    Sailing vessels for weekend use currently incl...      1.000000   \n",
      "4    UCSC Student Membership: $45\\nNon-Student Quar...      1.000000   \n",
      "..                                                 ...           ...   \n",
      "102  If you don't satisfy general education require...      1.000000   \n",
      "103  UC Transfer Admission Planner (UC TAP) is an o...      1.000000   \n",
      "104  For fall quarter acceptance, notices are sent ...      1.000000   \n",
      "105  Undergraduate students enrolled at UCSC may en...      1.000000   \n",
      "106  Advisers are available to answer your question...      0.833333   \n",
      "\n",
      "     answer_relevancy  context_recall  context_precision  \n",
      "0            0.996812        0.000000           0.556090  \n",
      "1            0.940775        0.333333           0.243590  \n",
      "2            0.954697        0.000000           0.196199  \n",
      "3            0.948260        0.500000           0.560462  \n",
      "4            0.908443        1.000000           0.238073  \n",
      "..                ...             ...                ...  \n",
      "102          0.945057        1.000000           0.267857  \n",
      "103          0.953325        0.000000           0.100000  \n",
      "104          0.952401        0.000000           1.000000  \n",
      "105          0.937296        0.500000           1.000000  \n",
      "106          1.000000        0.666667           0.833333  \n",
      "\n",
      "[107 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# === COMPREHENSIVE RAGAS EVALUATION ===\n",
    "# Execute multi-metric evaluation to assess Hybrid RAG performance\n",
    "# Each metric evaluates a different aspect of RAG system quality\n",
    "\n",
    "print(\"Starting comprehensive RAGAS evaluation...\")\n",
    "\n",
    "# Define evaluation metrics:\n",
    "# - faithfulness: How well answers are grounded in retrieved context\n",
    "# - answer_relevancy: How relevant answers are to the questions\n",
    "# - context_recall: How well retrieval captures relevant information\n",
    "# - context_precision: How precise the retrieved context is\n",
    "metrics = [faithfulness, answer_relevancy, context_recall, context_precision]\n",
    "\n",
    "# Configure evaluation settings for robust results\n",
    "run_cfg = RunConfig(\n",
    "    # timeout=60,        # Maximum time per evaluation\n",
    "    max_retries=10       # Retry failed evaluations for robustness\n",
    "    # max_wait=180,      # Maximum wait time\n",
    "    # max_workers=2      # Parallel evaluation workers\n",
    ")\n",
    "\n",
    "print(\"Running evaluation (this may take several minutes)...\")\n",
    "\n",
    "# Execute the comprehensive evaluation\n",
    "results = evaluate(\n",
    "    dataset=ragas_dataset,\n",
    "    metrics=metrics,\n",
    "    run_config=run_cfg,\n",
    "    raise_exceptions=False  # Continue evaluation even if some items fail\n",
    ")\n",
    "\n",
    "# Display detailed results\n",
    "results_df = results.to_pandas()\n",
    "print(\"\\nDetailed Evaluation Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2336b-5a87-4cab-b87d-dff414521d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores:\n",
      "faithfulness         0.881475\n",
      "answer_relevancy     0.827332\n",
      "context_recall       0.535358\n",
      "context_precision    0.468474\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# === CALCULATE AND DISPLAY AVERAGE PERFORMANCE ===\n",
    "# Compute overall system performance across all evaluation metrics\n",
    "# This provides a summary view of Hybrid RAG effectiveness\n",
    "\n",
    "print(\"Computing average performance metrics...\")\n",
    "\n",
    "# Calculate mean scores across all evaluated examples\n",
    "avg_scores = results_df[[\n",
    "    \"faithfulness\",        # Average grounding in retrieved context\n",
    "    \"answer_relevancy\",     # Average relevance to user questions  \n",
    "    \"context_recall\",       # Average retrieval completeness\n",
    "    \"context_precision\"     # Average retrieval accuracy\n",
    "]].mean()\n",
    "\n",
    "print(\"\\nHybrid RAG Average Performance:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, score in avg_scores.items():\n",
    "    print(f\"{metric:20s}: {score:.3f}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\nMetric Interpretation:\")\n",
    "print(\"• Faithfulness: How well answers stick to retrieved facts\")\n",
    "print(\"• Answer Relevancy: How well answers address the questions\")  \n",
    "print(\"• Context Recall: How completely relevant info is retrieved\")\n",
    "print(\"• Context Precision: How accurately irrelevant info is filtered\")\n",
    "\n",
    "avg_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
